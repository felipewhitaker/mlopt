---
theme: border
marp: true
size: 4:3
title: A brief explanation on Neural Networks and Learning
math: mathjax
author: Felipe Whitaker <felipewhitaker+mlopt@gmail.com>
footnote: "Disclaimer: the code was checked by a human, but written by a Neural Network"
---

# Neural Networks

## A brief explanation on Neural Networks and Learning

<!-- _footnote: "Author: Felipe Whitaker, 2023.1" -->

---

# Linear Regression

$y = f(x) = \beta_0 + \beta_1 \cdot x + \epsilon$

---

# Closed Form

---

# Gradient Descent

---

# Feature Engineering

TODO add image

$y = \beta_0 + \beta_1 * sin(x)$

---

# Activation Functions

TODO nonlinear activation fuction

---

# Breadth and Depth

TODO universal approximator
TODO problems to generalization (foreshadow trees)

---

# Trees

---

# Classification

TODO loss metric
TODO xor example

---

# Stochastic Gradient Descent

TODO [go in depth](https://thenumb.at/Autodiff/)
TODO other ideas: momentum + other optimizers

---

# Implementation

TODO flux
TODO epochs, batches, etc.

---

# Loss Visualization

TODO heatmap of loss

---

# Architectures

TODO structured vs non-structured data / how to capture signal

---

# Convolutional Neural Networks

---

# Recurrent Neural Networks

---

# References
